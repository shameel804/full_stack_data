[
    {
        "title": "The TDD Mantra üßò",
        "ques": "Test-Driven Development (TDD) relies on a strict cycle. Arrange the following three steps in the correct order:\n1.  **Refactor** code to improve structure.\n2.  Write a **failing test** for a new feature.\n3.  Write the **minimal code** necessary to pass the test.",
        "answer": {
            "type": "text",
            "content": "The correct order (The TDD Cycle) is:\n1.  **Write a failing test** (Red)\n2.  **Write the minimal code** to pass the test (Green)\n3.  **Refactor** the code (Refactor)"
        },
        "explanation": "This cycle is often referred to as **Red-Green-Refactor**. Starting with a **failing test** ensures that you are actually testing the new functionality and that the test is valid."
    },
    {
        "title": "Why Fail First? üõë",
        "ques": "In TDD, why is it mandatory to run the test and watch it fail **before** writing any implementation code?",
        "answer": {
            "type": "text",
            "content": "Watching the test fail first confirms two things:\n1.  **The test is working:** It proves that the test is actually checking for the missing feature (avoiding false positives).\n2.  **The feature is missing:** It confirms that the current codebase verifies the absence of the functionality you are about to add."
        },
        "explanation": "If a test passes before you write the code, it's a generic or incorrectly written test. Seeing it **fail** gives you confidence that the test is meaningful."
    },
    {
        "title": "Minimal Implementation Code ü§è",
        "ques": "You have a test that expects a function `add(a, b)` to return `4` when called with `2, 2`. \n\nBefore implementing the real logic (`return a + b`), what is the most **minimal code** you could write just to make this specific test pass?",
        "answer": {
            "type": "code",
            "lang": "python",
            "content": "def add(a, b):\n    # Minimal code to pass the specific test case add(2, 2) == 4\n    return 4"
        },
        "explanation": "This extreme simplicity forces you to write **another test case** (e.g., `add(3, 5)`) to force the implementation to become more general. This ensures code is only written when a test requires it."
    },
    {
        "title": "TDD vs. Unit Testing ü•ä",
        "ques": "How is **TDD** different from simply writing **Unit Tests** after the code is written?",
        "answer": {
            "type": "text",
            "content": "*   **Unit Testing:** Typically done **after** code is written to verify it works. The code design is already set.\n*   **TDD:** Tests are written **before** the code. This drives the **design** of the code itself, ensuring it is testable, modular, and meets the exact requirements defined by the test."
        },
        "explanation": "TDD is primarily a **design process** that produces tests as a byproduct, whereas traditional unit testing is a **verification process**."
    },
    {
        "title": "The 'Baby Steps' Approach üë∂",
        "ques": "What does the concept of \"Baby Steps\" mean in the context of the TDD cycle?",
        "answer": {
            "type": "text",
            "content": "**Baby Steps** means breaking down the problem into the smallest possible units of functionality. Instead of trying to implement a complex algorithm in one go, you write a test for the simplest case (e.g., an empty list input), make it pass, and then write a slightly more complex test."
        },
        "explanation": "Taking **baby steps** keeps the feedback loop short and manageable. If a test fails, you know exactly what small change caused it, making debugging trivial."
    }
]